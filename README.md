# CS-156 Homework code
Here is my code for the CS-156 homework. The interesting parts, such as algorithms are in public/algorithms.py so please check that out.

       <h4>1 The Learning Problem</h4>
        <h5>1.1 Problem Setup</h5>
        <p><strong>1.1.1 Components of Learning</strong></p>
        
        <p><strong>Training examples</strong> $(x_1,y_1),(x_2,y_2),...,(x_N,y_N)$. Here $x_N$ is $n^{th}$ input and $y_N$ is its output, as if generated by the ideal target function.</p>
        <p><strong>Unknown target function</strong> $f: \mathcal{X} \mapsto \mathcal{Y}$.</p>
        <p><strong>Hypothesis set</strong> $\mathcal{H}$. The set includes all our candidate functions.</p>
        <p><strong>Learning Algorithm</strong> $\mathcal{A}$. Machine learning algorithm  that is fed with training data and hypothesis set  to provide final hypothesis.</p>
        <p><strong>Final Hypothesis</strong> $g \approx f$. We end up with an estimated function <strong>$g$</strong> that best matches $f$ on the training examples, with the hope of it generalising for new data.</p>
        <br>
        <p><strong>1.1.2 Perceptron Learning Model</strong></p>
        <p>The book illustrates perceptron model with the credit approval</p>
        $$Approve\: \: if \qquad \sum^d_{i=1}{w_ix_i} > treshold $$
        $$Deny\: \: if \qquad \sum^d_{i=1}{w_ix_i} < treshold $$
        
        <p>More compactly the formula can be written</p>
        
        $$h(\textbf{x}) = sign\Bigg(\sum^d_{i=1}w_ix_i -treshold \Bigg)$$
        
        <p>treshold and the weights $w_i$ are the variables that we will work on to minimize the number of misclassifications and $x_i$ are the inputs. $h(x)$ can either be +1 or -1.</p>
        <p>It is most common to add the treshold to the weights vector as $w_0$ component $\textbf{w} = (w_0\  w_1\ w_2\ ...\  w_N)^T$. This requires adding $x_0 = 1$ component to our inputs $\textbf{x} = (x_0\ x_1\ x_2\ ...\ x_N)^T$. After such rearrangements we can express our hypothesis in a more compact vector notation.</p>
        $$h(\textbf{x}) = sign\big(\textbf{w}^T\textbf{x} \big) $$
        <!-- add perceptron image mislassified -> classified data-->
        <br>
        <p><i>Perceptron Learning Algorithm (PLA)</i> - iterate over the data points $(x_1,y_1),(x_2,y_2),...,(x_N,y_N)$ and find a misclassified point. Then adjust weights to classify correctly.</p>
        
        $$w(t+1) = w(t) + y(t)x(t) $$
        
        <p>We can imagine the perceptron model as a vector dot product between $w$ and $x$, if the hypothesis is +1, then the angle between them is $ < 90 ^ \circ $, if -1, then $ > 90 ^ \circ $ the algorithm sums vectors as shown above to change the angle, hence the sign.</p>
        
